# 视觉里程计实验手册

## 特征点匹配
基于双目两张图像，估计出视差图和点云图，效果如下：

<img src="./orb_keypoints_match.png" style="width: 800px; height: auto; border-radius: 8px;">

## 从2d2d匹配点估计位姿
- 基于ORB特征点检测和匹配，基于对极几何关系求本质矩阵 findEssentialMat
- 从本质矩阵中恢复出位姿信息，通过额外的点从可能解中确认具体的解

## 从3d2d匹配点估计位姿
- 调用OpenCV 的 PnP 求解，可选择EPNP，DLS等方法，主要用于单帧的位姿估计，精度没有后两者高
- 基于高斯牛顿法的非线性最小二乘，主要用具局部多帧的位姿和地图点的联合优化和全局的位姿和地图点优化, 也有将地图点作为固定的常量，专门优化位姿的场景（pose only）
    - Full BA: 最高精度，最大计算量
    - Pose-Only: 实时性好，用于重定位/回环
- 基于g2o库的bundle ajustment进行重投影误差最小化(最小二乘)

## 从RGB-D构成的3d3d匹配点中估计位姿
- ICP，通过奇异值分解求得
- bundle ajuestment，3D-3D场景下，我们处理的不再是图像像素，而是空间中的三维点。因此，Bundle Adjustment的目标函数从最小化“重投影误差”变为了最小化“三维点坐标的对齐误差”，但其底层的非线性优化框架（如高斯牛顿法或LM法）是完全一致的。