# RT1

## RT-1要点
- 视觉信息Univeral Sentence Encoder，RT-1中的Universal Sentence Encoder如何实现，基于bert的cls输出吗，还是其他的wordembedding的平均等实现
- FiLM的实现是和视觉信号进行一个类似于zero conv的融合操作，而不是基于cross attention机制来进行实现的
- TokenLearn类似于vallina attention机制实现token的压缩
- 单个图片和instruction融合生成tokens后再时序拼接
- 估计的是action的绝对的value(和RT-2不同)
- RT1数据集简介

### rt-1中7个机械臂运动变量+3个底盘运动变量都可以量化为256个bins，然后还有一个模式切换变量，只有三种状态，是否分类的时候就分类为3类，也就是10输出，分别连接256个分类任务，1个输出接出3个分类任务，这和一般的token表示的含义还不太一样，因为每个输出的token随都是256类，但结果含义表示不同？不过可以统一以token来看对吗？

## 参考资料
- https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/
- https://github.com/google-research/robotics_transformer
- pytorch版本复现： https://github.com/lucidrains/robotic-transformer-pytorch 